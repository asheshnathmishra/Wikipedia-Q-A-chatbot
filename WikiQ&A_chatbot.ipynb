{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a99318",
   "metadata": {},
   "source": [
    "## Importing the data from Wikipedia page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff892d",
   "metadata": {},
   "source": [
    "I am taking the wikipedia page  https://en.wikipedia.org/wiki/Atal_Tunnel for our Q&A bot context. However, this link can be replaced with any link as per user's choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba43270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaForQuestionAnswering\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e196cf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extracted Successfully:\n",
      "\n",
      "\n",
      "Atal Tunnel (also known as Rohtang Tunnel),[1] named after former Prime Minister of India, Atal Bihari Vajpayee is a highway tunnel built under the Rohtang Pass in the eastern Pir Panjal range of the Himalayas on the National Highway 3 in Himachal Pradesh, India.[1][2] At a length of 9.02 km, it is the highest highway single-tube tunnel above 10,000 feet (3,048 m) in the world.[3][4][5] With the existing Atal Tunnel and after the completion of under-construction Shinku La Tunnel, which is targ\n"
     ]
    }
   ],
   "source": [
    "def extract_wikipedia_data(wikipedia_link):\n",
    "    \"\"\"\n",
    "    Extracts data from a Wikipedia page and returns the content.\n",
    "\n",
    "    Parameters:\n",
    "    - wikipedia_link (str): The link to the Wikipedia page.\n",
    "\n",
    "    Returns:\n",
    "    - wikipedia_data (str): The extracted data from the Wikipedia page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch HTML content from the Wikipedia page\n",
    "        page = urllib.request.urlopen(wikipedia_link)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        # Extract text data\n",
    "        text_data = []\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            text_data.append(paragraph.text)\n",
    "\n",
    "        # Combine paragraphs into a single string\n",
    "        wikipedia_data = '\\n'.join(text_data)\n",
    "\n",
    "        return wikipedia_data\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions, e.g., invalid URL or network issues\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "#execution\n",
    "wikipedia_link = 'https://en.wikipedia.org/wiki/Atal_Tunnel'\n",
    "extracted_data = extract_wikipedia_data(wikipedia_link)\n",
    "wikipedia_text = extracted_data\n",
    "\n",
    "if extracted_data:\n",
    "    print(\"Data Extracted Successfully:\")\n",
    "    print(extracted_data[:500])  # Displaying the first 500 characters as a sample\n",
    "else:\n",
    "    print(\"Failed to extract data from the Wikipedia page.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7a305",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9eb24",
   "metadata": {},
   "source": [
    "If I print the extracted data from wikipedia as is, there are a lot of HTML tags and line breaks come along with the text, which is why I have wrote the clean_wikipedia_text method to remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f231d1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Atal Tunnel (also known as Rohtang Tunnel), named after former Prime Minister of India, Atal Bihari Vajpayee is a highway tunnel built under the Rohtang Pass in the eastern Pir Panjal range of the Himalayas on the National Highway 3 in Himachal Pradesh, India. At a length of 9.02 km, it is the highest highway single-tube tunnel above 10,000 feet (3,048 m) in the world. With the existing Atal Tunnel and after the completion of under-construction Shinku La Tunnel, which is targeted to be complet\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_wikipedia_text(text):\n",
    "    \"\"\"\n",
    "    Cleans Wikipedia text by removing HTML tags, hyper references, \n",
    "    paragraph numbers, <s> tags, newline characters, and other unwanted elements.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The raw text from a Wikipedia page or similar source.\n",
    "    \n",
    "    Returns:\n",
    "    str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove HTML tags\n",
    "    clean_text = re.sub('<.*?>', '', text)\n",
    "\n",
    "    # Remove paragraph numbers like [1], [2], etc.\n",
    "    clean_text = re.sub(r'\\[\\d+\\]', '', clean_text)\n",
    "\n",
    "    # Replace newline characters with a space\n",
    "    clean_text = clean_text.replace('\\n', ' ')\n",
    "\n",
    "    # Remove <s> tags\n",
    "    clean_text = clean_text.replace('<s>', '').replace('</s>', '')\n",
    "\n",
    "    # Additional cleaning steps can be added here if needed\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "# execution\n",
    "raw_text = wikipedia_text\n",
    "cleaned_text = clean_wikipedia_text(raw_text)\n",
    "print(cleaned_text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f87d7",
   "metadata": {},
   "source": [
    "## Q&A Bot using Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc4d03e",
   "metadata": {},
   "source": [
    "Now as we do not have a data set of Q&A answer on which we can train our model, I have taken a pre trained transformer model (Deep learning Model as per our case study) to answer questions about the text in wikipedia page without specific fine-tuning on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f6dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('deepset/roberta-base-squad2')\n",
    "model = RobertaForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4e3978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1738 > 512). Running this sequence through the model will result in indexing errors\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  9.02 km\n"
     ]
    }
   ],
   "source": [
    "def answer_question(model, tokenizer, question, context, max_length=512):\n",
    "    # Tokenize the question to understand its length\n",
    "    question_tokens = tokenizer.encode(question, add_special_tokens=True)\n",
    "\n",
    "    # Initialize variables to store processed segments\n",
    "    segments = []\n",
    "\n",
    "    # Tokenize the context and split into segments\n",
    "    remaining_tokens = tokenizer.encode(context, add_special_tokens=False, add_prefix_space=True)\n",
    "    while remaining_tokens:\n",
    "        segment_tokens = []\n",
    "        while remaining_tokens and len(question_tokens) + len(segment_tokens) < max_length:\n",
    "            segment_tokens.append(remaining_tokens.pop(0))\n",
    "        segments.append(tokenizer.decode(segment_tokens))\n",
    "\n",
    "    # Iterate over each segment and look for the answer\n",
    "    for segment in segments:\n",
    "        inputs = tokenizer.encode_plus(question, segment, add_special_tokens=True, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "        input_ids = inputs['input_ids'].to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "        answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "        answer_start = torch.argmax(answer_start_scores)\n",
    "        answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "        # Check if a valid answer is found in this segment\n",
    "        if answer_start < answer_end:\n",
    "            return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end]))\n",
    "\n",
    "    return \"Sorry, I could not find an answer in the text.\"\n",
    "\n",
    "# Execution\n",
    "context = cleaned_text\n",
    "question = \"What is the length of Atal Tunnel?\"\n",
    "\n",
    "answer = answer_question(model, tokenizer, question, context)\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0806b55",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f6ed917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  9.02 km\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What is the size of Atal tunnel?\"\n",
    "\n",
    "answer = answer_question(model, tokenizer, question, context)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c11944a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Himachal Pradesh, India\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Where is Atal tunnel?\"\n",
    "\n",
    "answer = answer_question(model, tokenizer, question, context)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64664045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Atal Bihari Vajpayee\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Who built it?\"\n",
    "\n",
    "answer = answer_question(model, tokenizer, question, context)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffefe707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  3,100 metres\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"what is the elevation of atal tunnel?\"\n",
    "\n",
    "answer = answer_question(model, tokenizer, question, context)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7325f656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  ₹3,200 crore (US$438 million).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"what is the cost?\"\n",
    "\n",
    "answer = answer_question(model, tokenizer, question, context)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25a97cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Border Roads Organisation (BRO) under Ministry of Defence\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"By whom it was completed?\"\n",
    "\n",
    "answer = answer_question(model, tokenizer, question, context)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae8a4d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Himachal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"which state's tourism will it boost?\"\n",
    "\n",
    "answer = answer_question(model, tokenizer, question, context)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7782d5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  road blockades, avalanches, and traffic snarls\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"what challanges did it face?\"\n",
    "\n",
    "answer = answer_question(model, tokenizer, question, context)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5792d01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  it is the highest highway single-tube tunnel above 10,000 feet (3,048 m) in the world\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"what are the specififcations of atal tunnel?\"\n",
    "\n",
    "answer = answer_question(model, tokenizer, question, context)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347a6a1",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad765e5e",
   "metadata": {},
   "source": [
    "To Evaluate the model, as we don not have  a existing set for testing, We need to create a evaluation test consiting of questions and correct answers then get the answer from Q&A bot, match Q&A bot answers with actual answer and calculate the accuracy (by f1 score of exact match or any other evaluation methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cbded2",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba93f2",
   "metadata": {},
   "source": [
    "I have taken reference from roberta huggingface website to import and execute the model, doc strings i have written using chatgpt py simply pasting my methods,  Ihave taken references from stackoverflow to fix few bugs faced during development. I haven't copied the entire code directly from any source whatsoever. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
